{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Parametric model building\n",
    "As with any fitting exercise, the first step is to understand the format of the input data, explore its contents and construct a model. The python script which performs the model construction is `construct_models_part1.py`. The code from this file has been copied to this notebook.\n",
    "\n",
    "### Signal modelling\n",
    "Firstly, we will construct a model to fit the signal (H$\\rightarrow\\gamma\\gamma$) mass peak using a Monte Carlo simulation sample of gluon-gluon fusion production (ggH) events with $m_H=125$ GeV. This production mode has the largest cross section in the SM, and the LO Feynman diagram is shown below.\n",
    "\n",
    "<img src=\"plots/part1_feynman.png\" width=\"300\"/>\n",
    "\n",
    "There has already been a dedicated selection performed on the events to increase the signal-to-background ratio (e.g. using some ML event classifier). Events passing this selection enter the analysis category, Tag0. Events entering Tag0 are used for the parametric fitting of the $m_{\\gamma\\gamma}$ spectrum. \n",
    "\n",
    "The events are stored in a ROOT `TTree`, where the diphoton mass, `CMS_hgg_mass`, and the event weight, `weight`, are saved. Let's begin by loading the MC, and converting the `TTree` data into `RooDataSet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = ROOT.TFile(\"mc_part1.root\",\"r\")\n",
    "# Load TTree\n",
    "t = f.Get(\"ggH_Tag0\")\n",
    "\n",
    "# Define mass and weight variables\n",
    "mass = ROOT.RooRealVar(\"CMS_hgg_mass\", \"CMS_hgg_mass\", 125, 100, 180)\n",
    "weight = ROOT.RooRealVar(\"weight\",\"weight\",0,0,1)\n",
    "\n",
    "# Convert to RooDataSet\n",
    "mc = ROOT.RooDataSet(\"ggH_Tag0\",\"ggH_Tag0\", t, ROOT.RooArgSet(mass,weight), \"\", \"weight\" )\n",
    "\n",
    "# Lets plot the signal mass distribution\n",
    "can = ROOT.TCanvas()\n",
    "plot = mass.frame()\n",
    "mc.plotOn(plot)\n",
    "plot.Draw()\n",
    "can.Update()\n",
    "can.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows a peak centred on the Higgs mass at 125 GeV. Let's use a simple Gaussian to fit the peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Introduce a RooRealVar into the workspace for the Higgs mass\n",
    "MH = ROOT.RooRealVar(\"MH\", \"MH\", 125, 120, 130 )\n",
    "MH.setConstant(True)\n",
    "\n",
    "# Signal peak width\n",
    "sigma = ROOT.RooRealVar(\"sigma_ggH_Tag0\", \"sigma_ggH_Tag0\", 2, 1, 5)\n",
    "\n",
    "# Define the Gaussian with mean=MH and width=sigma\n",
    "model = ROOT.RooGaussian( \"model_ggH_Tag0\", \"model_ggH_Tag0\", mass, MH, sigma ) \n",
    "\n",
    "# Fit Gaussian to MC events and plot\n",
    "model.fitTo(mc,ROOT.RooFit.SumW2Error(True))\n",
    "\n",
    "can = ROOT.TCanvas()\n",
    "plot = mass.frame()\n",
    "mc.plotOn(plot)\n",
    "model.plotOn( plot, ROOT.RooFit.LineColor(2) )\n",
    "plot.Draw()\n",
    "can.Update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets plot the canvas for the Gaussian fit\n",
    "can.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a good fit!\n",
    "\n",
    "* Do you understand the output from the `fitTo` command (the mimimization)? From now on we will add the option `ROOT.RooFit.PrintLevel(-1)` when fitting the models to surpress the minimizer output. \n",
    "\n",
    "But what if the mean of the model does not correspond directly to the Higgs boson mass i.e. there are some reconstruction effects. Let's instead define the mean of the model as:\n",
    "\n",
    "$$\\mu = m_H + \\delta$$\n",
    "\n",
    "and we can fit for $\\delta$ in the model construction. For this we introduce a `RooFormulaVar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dMH = ROOT.RooRealVar(\"dMH_ggH_Tag0\", \"dMH_ggH_Tag0\", 0, -1, 1 )\n",
    "mean = ROOT.RooFormulaVar(\"mean_ggH_Tag0\", \"mean_ggH_Tag0\", \"(@0+@1)\", ROOT.RooArgList(MH,dMH))\n",
    "model = ROOT.RooGaussian( \"model_ggH_Tag0\", \"model_ggH_Tag0\", mass, mean, sigma )\n",
    "\n",
    "# Fit the new model with a variable mean\n",
    "model.fitTo(mc,ROOT.RooFit.SumW2Error(True),ROOT.RooFit.PrintLevel(-1))\n",
    "\n",
    "# Model is parametric in MH. Let's show this by plotting for different values of MH\n",
    "can = ROOT.TCanvas()\n",
    "plot = mass.frame()\n",
    "MH.setVal(120)\n",
    "model.plotOn( plot, ROOT.RooFit.LineColor(2) )\n",
    "MH.setVal(125)\n",
    "model.plotOn( plot, ROOT.RooFit.LineColor(3) )\n",
    "MH.setVal(130)\n",
    "model.plotOn( plot, ROOT.RooFit.LineColor(4) )\n",
    "plot.Draw()\n",
    "can.Update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets plot the canvas for the Gaussian fit\n",
    "can.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now save the model inside a `RooWorkspace`. Combine will load this model when performing the fits. Crucially, we need to freeze the fit parameters of the signal model, otherwise they will be freely floating in the final results extraction. \n",
    "\n",
    "* This choice of setting the shape parameters to constant means we believe our MC will perfectly model the Higgs boson events in data. Is this the case? How could we account for the MC mis-modelling in the fit? (See Part 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MH.setVal(125)\n",
    "dMH.setConstant(True)\n",
    "sigma.setConstant(True)\n",
    "\n",
    "f_out = ROOT.TFile(\"workspace_sig.root\", \"RECREATE\")\n",
    "w_sig = ROOT.RooWorkspace(\"workspace_sig\",\"workspace_sig\")\n",
    "getattr(w_sig, \"import\")(model)\n",
    "w_sig.Print()\n",
    "w_sig.Write()\n",
    "f_out.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't be afraid of the RooWorkspace: we can easily check its contents with member functions e.g\n",
    "w_sig.var(\"sigma_ggH_Tag0\").getVal()\n",
    "\n",
    "# Try adding some code here to play around with the contents of the workspace...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully constructed a parametric model to fit the shape of the signal peak. But we also need to know the yield/normalisation of the ggH signal process. In the SM, the ggH event yield is equal to:\n",
    "\n",
    "$$ N = \\sigma_{ggH} \\cdot \\mathcal{B}^{\\gamma\\gamma} \\cdot \\epsilon \\cdot \\mathcal{L}$$\n",
    "\n",
    "Where $\\sigma_{ggH}$ is the SM ggH cross section, $\\mathcal{B}^{\\gamma\\gamma}$ is the SM branching fraction of the Higgs boson to two photons, $\\epsilon$ is the efficiency factor and corresponds to the fraction of the total ggH events landing in the Tag0 analysis category. Finally $\\mathcal{L}$ is the integrated luminosity.\n",
    "\n",
    "In this example, the ggH MC events are normalised **before any selection is performed** to $\\sigma_{ggH} \\cdot \\mathcal{B}^{\\gamma\\gamma}$, taking the values from the [LHCHWG twiki](https://twiki.cern.ch/twiki/bin/view/LHCPhysics/LHCHWG#Production_cross_sections_and_de). Note this does not include the lumi scaling, which may be different to what you have in your own analyses! We can then calculate the efficiency factor, $\\epsilon$, by taking the sum of weights in the MC dataset and dividing through by $\\sigma_{ggH} \\cdot \\mathcal{B}^{\\gamma\\gamma}$. This will tell us what fraction of ggH events land in Tag0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define SM cross section and branching fraction values\n",
    "xs_ggH = 48.58 #in [pb]\n",
    "br_gamgam = 2.7e-3\n",
    "\n",
    "# Calculate the efficiency and print output\n",
    "sumw = mc.sumEntries()\n",
    "eff = sumw/(xs_ggH*br_gamgam)\n",
    "print(\"Efficiency of ggH events landing in Tag0 is: %.2f%%\"%(eff*100))\n",
    "\n",
    "# Calculate the total yield (assuming full Run 2 lumi) and print output\n",
    "lumi = 138000\n",
    "N = xs_ggH*br_gamgam*eff*lumi\n",
    "print(\"For 138fb^-1, total normalisation of signal is: N = xs * br * eff * lumi = %.2f events\"%N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we find 1% of all ggH events enter Tag0. And the total expected yield of ggH events in Tag0 (with lumi scaling) is `181.01`. Lets make a note of this for later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background modelling\n",
    "In the H$\\rightarrow\\gamma\\gamma$ analysis we construct the background model directly from data. To avoid biasing our background estimate, we remove the signal region from the model construction and fit the mass sidebands. Let's begin by loading the data `TTree` and converting to a `RooDataSet`. We will then plot the mass sidebands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = ROOT.TFile(\"data_part1.root\",\"r\")\n",
    "t = f.Get(\"data_Tag0\")\n",
    "\n",
    "# Convert TTree to a RooDataSet\n",
    "data = ROOT.RooDataSet(\"data_Tag0\", \"data_Tag0\", t, ROOT.RooArgSet(mass), \"\", \"weight\")\n",
    "\n",
    "# Define mass sideband ranges on the mass variable: 100-115 and 135-180\n",
    "n_bins = 80\n",
    "binning = ROOT.RooFit.Binning(n_bins,100,180)\n",
    "mass.setRange(\"loSB\", 100, 115 )\n",
    "mass.setRange(\"hiSB\", 135, 180 )\n",
    "mass.setRange(\"full\", 100, 180 )\n",
    "fit_range = \"loSB,hiSB\"\n",
    "\n",
    "# Plot the data in the mass sidebands\n",
    "can = ROOT.TCanvas()\n",
    "plot = mass.frame()\n",
    "data.plotOn( plot, ROOT.RooFit.CutRange(fit_range), binning )\n",
    "plot.Draw()\n",
    "can.Update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets plot the canvas for the Gaussian fit\n",
    "can.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye, it looks like an exponential function would fit the data sidebands well. Let's construct the background model using a RooExponential and fit the data sidebands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = ROOT.RooRealVar(\"alpha\", \"alpha\", -0.05, -0.2, 0 )\n",
    "model_bkg = ROOT.RooExponential(\"model_bkg_Tag0\", \"model_bkg_Tag0\", mass, alpha )\n",
    "\n",
    "# Fit model to data sidebands\n",
    "model_bkg.fitTo( data, ROOT.RooFit.Range(fit_range), ROOT.RooFit.PrintLevel(-1) )\n",
    "\n",
    "# Let's plot the model fit to the data\n",
    "can = ROOT.TCanvas()\n",
    "plot = mass.frame()\n",
    "# We have to be careful with the normalisation as we only fit over sidebands\n",
    "# First do an invisible plot of the full data set\n",
    "data.plotOn( plot, binning, ROOT.RooFit.MarkerColor(0), ROOT.RooFit.LineColor(0) )\n",
    "model_bkg.plotOn( plot, ROOT.RooFit.NormRange(fit_range), ROOT.RooFit.Range(\"full\"), ROOT.RooFit.LineColor(2))\n",
    "data.plotOn( plot, ROOT.RooFit.CutRange(fit_range), binning )\n",
    "plot.Draw()\n",
    "can.Update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets plot the canvas for the Gaussian fit\n",
    "can.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the background model is extracted from data, we want to introduce a freely floating normalisation term. We use the total number of data events (including in the signal region) as the initial prefit value of this normalisation object i.e. assuming no signal in the data. The syntax to name this normalisation object is `{model}_norm` which will the be picked up automatically by combine. Note we also allow the shape parameter to float in the final fit to data (by not setting to constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm = ROOT.RooRealVar(\"model_bkg_Tag0_norm\", \"Number of background events in Tag0\", data.numEntries(), 0, 3*data.numEntries() )\n",
    "alpha.setConstant(False)\n",
    "\n",
    "# Let's then save the background model, the normalisation object, and the data distribution to a new `RooWorkspace\n",
    "f_out = ROOT.TFile(\"workspace_bkg.root\", \"RECREATE\")\n",
    "w_bkg = ROOT.RooWorkspace(\"workspace_bkg\",\"workspace_bkg\")\n",
    "getattr(w_bkg, \"import\")(data)\n",
    "getattr(w_bkg, \"import\")(norm)\n",
    "getattr(w_bkg, \"import\")(model_bkg)\n",
    "w_bkg.Print()\n",
    "w_bkg.Write()\n",
    "f_out.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datacard\n",
    "The model workspaces have now been constructed. But before we can run any fits in combine we need to build the so-called **datacard**. This is a text file which defines the different processes entering the fit and their expected yields, and maps these processes to the corresponding (parametric) models. We also store information on the systematic uncertainties in the datacard (see part 3). Given the low complexity of this example, the datacard is reasonably short. The datacard for this section is titled `datacard_part1.txt`. Take some time to understand the different lines. In particular, the values for the process normalisations:\n",
    "\n",
    "* Where does the signal (ggH) normalisation come from?\n",
    "* Why do we use a value of 1.0 for the background model normalisation in this analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's open the datacard and take a look\n",
    "with open(\"datacard_part1.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(\"\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile the datacard we run the following command (in bash), using a value of the Higgs mass of 125.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "text2workspace.py datacard_part1.txt -m 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This compiles the datacard into a RooWorkspace, effectively building the likelihood function. Try opening the compiled workspace (`datacard_part1.root`) and print the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = ROOT.TFile(\"datacard_part1.root\")\n",
    "w = f.Get(\"w\")\n",
    "w.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you understand what the objects in the workspace are? What does the variable `r` correspond to? Try (verbose) printing the variable with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.var(\"r\").Print(\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: signal normalisation object\n",
    "In the example above, the signal model normalisation is input by hand in the datacard. We can instead define the signal normalisation components in the model in a similar fashion to the background model normalisation object. Let's build the cross section (ggH), branching fraction (H->gamgam), and efficiency variables. It's important to set these terms to be constant for the final fit to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_ggH = ROOT.RooRealVar(\"xs_ggH\", \"Cross section of ggH in [pb]\", 48.58 )\n",
    "br_gamgam = ROOT.RooRealVar(\"BR_gamgam\", \"Branching ratio of Higgs to gamma gamma\", 0.0027 )\n",
    "eff_ggH_Tag0 = ROOT.RooRealVar(\"eff_ggH_Tag0\", \"Efficiency for ggH events to land in Tag0\", eff )\n",
    "\n",
    "xs_ggH.setConstant(True)\n",
    "br_gamgam.setConstant(True)\n",
    "eff_ggH_Tag0.setConstant(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalisation component is then defined as the product of these three variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sig = ROOT.RooProduct(\"model_ggH_Tag0_norm\", \"Normalisation term for ggH in Tag 0\", ROOT.RooArgList(xs_ggH,br_gamgam,eff_ggH_Tag0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the syntax `{model}_norm` has been used so that combine will automatically assign this object as the normalisation for the model (`model_ggH_Tag0`). Firstly we need to save a new version of the signal model workspace with the normalisation term included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = ROOT.TFile(\"workspace_sig_with_norm.root\", \"RECREATE\")\n",
    "w_sig = ROOT.RooWorkspace(\"workspace_sig\",\"workspace_sig\")\n",
    "getattr(w_sig, \"import\")(model)\n",
    "getattr(w_sig, \"import\")(norm_sig)\n",
    "w_sig.Print()\n",
    "w_sig.Write()\n",
    "f_out.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to modify the datacard to account for this normalisation term. Importantly, the `{model}_norm` term in our updated signal model workspace does not contain the integrated luminosity. Therefore, the `rate` term in the datacard must be set equal to the integrated luminosity in [pb^-1] (as the cross section was defined in [pb]). The total normalisation for the signal model is then the product of the `{model}_norm` and the `rate` value. \n",
    "* You can find the example datacard here: `datacard_part1_with_norm.txt` with the signal normalisation object included. Check if it compiles successfully using `text2workspace`? If so, try printing out the contents of the workspace. Can you see the normalisation component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's open the datacard and take a look\n",
    "with open(\"datacard_part1_with_norm.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(\"\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Try compiling the datacard_part1_with_norm.txt datacard\n",
    "text2workspace.py datacard_part1_with_norm.txt -m 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: unbinned vs binned\n",
    "In a parametric analysis, the fit can be performed using a binned or unbinned likelihood function. The consequences of binned vs unbinned likelihoods were discussed in the [morning session](https://indico.cern.ch/event/1227742/contributions/5240048/). In combine, we can simply toggle between binned and unbinned fits by changing how the data set is stored in the workspace. In the example above, the data was saved as a `RooDataSet`. This means that an unbinned maximum likelihood function would be used.\n",
    "\n",
    "To switch to a **binned** maximum likelihood fit, we need to store the data set in the workspace as a `RooDataHist`. Let's first load the data as a `RooDataSet` as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ROOT.TFile(\"data_part1.root\",\"r\")\n",
    "t = f.Get(\"data_Tag0\")\n",
    "\n",
    "# Convert TTree to a RooDataSet\n",
    "data = ROOT.RooDataSet(\"data_Tag0\", \"data_Tag0\", t, ROOT.RooArgSet(mass, weight), \"\", \"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to set the number of bins in the observable and convert the data to a `RooDataHist`. In this example we will use 320 bins over the full mass range (0.25 GeV per bin). It is important that the binning is sufficiently granular so that we do not lose information in the data by switching to a binned likelihood fit. When fitting a signal peak over a background we want the bin width to be sufficiently smaller than the signal model mass resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bin number for mass variables\n",
    "mass.setBins(320)\n",
    "data_hist = ROOT.RooDataHist(\"data_hist_Tag0\", \"data_hist_Tag0\", mass, data)\n",
    "\n",
    "# Save the background model with the RooDataHist instead\n",
    "f_out = ROOT.TFile(\"workspace_bkg_binned.root\", \"RECREATE\")\n",
    "w_bkg = ROOT.RooWorkspace(\"workspace_bkg\",\"workspace_bkg\")\n",
    "getattr(w_bkg, \"import\")(data_hist)\n",
    "getattr(w_bkg, \"import\")(norm)\n",
    "getattr(w_bkg, \"import\")(model_bkg)\n",
    "w_bkg.Print()\n",
    "w_bkg.Write()\n",
    "f_out.Close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
