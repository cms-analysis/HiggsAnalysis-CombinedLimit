{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Multi-signal model\n",
    "In reality, there are multiple Higgs boson processes which contribute to the total signal model, not only ggH. This section will explain how we can add an additional signal process (VBF) into the fit. Following this, we will add a second analysis category (Tag1), which has a higher purity of VBF events. To put this in context, the selection for Tag1 may require two jets with a large pseudorapidity separation and high invariant mass, which are typical properties of the VBF topology. By including this additional category with a different relative yield of VBF to ggH production, we are able to simultaneously constrain the rate of the two production modes.\n",
    "\n",
    "In the SM, the VBF process has a cross section which is roughly 10 times smaller than the ggH cross section. This explains why we need to use certain features of the event to boost the purity of VBF events. The LO Feynman diagram for VBF production is shown below.\n",
    "\n",
    "<img src=\"plots/part6_feynman.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the models\n",
    "Firstly, lets build the necessary inputs for this section the following code blocks. The code uses everything we have learnt in the previous sections:\n",
    "\n",
    "1) Signal models (Gaussians) are built separately for each process (ggH and VBF) in each analysis category (Tag0 and Tag1). This uses separate `TTrees` for each contribution in the `mc_part6.root` file. The mean and width of the Gaussians include the effect of the parametric shape uncertainties, `nuisance_scale` and `nuisance_smear`. Each signal model is normalised according to the following equation, where $\\epsilon_{ij}$ labels the fraction of process, $i$ (=ggH,VBF), landing in analysis category, $j$ (=Tag0,Tag1), and $\\mathcal{L}$ is the integrated luminosity (defined in the datacard).\n",
    "\n",
    "$$ N_{ij} = \\sigma_i \\cdot \\mathcal{B}^{\\gamma\\gamma} \\cdot \\epsilon_{ij} \\cdot \\mathcal{L}$$\n",
    "\n",
    "2) A background model is constructed for each analysis category by fitting the mass sidebands in data. The input data is stored in the `data_part6.root` file. The models are `RooMultiPdfs` which contain an exponential, a 4th-order Chebychev polynomial and a power law function. The shape parameters and normalisation terms of the background models are freely floating in the final fit.\n",
    "* Have a look through the code and try to understand all parts of the model construction. When you are happy, go ahead and construct the models by running the blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal modelling\n",
    "# Open ROOT file containing trees for both ggH and VBF\n",
    "f = ROOT.TFile(\"mc_part6.root\")\n",
    "\n",
    "# Define mass and weight variables\n",
    "mass = ROOT.RooRealVar(\"CMS_hgg_mass\", \"CMS_hgg_mass\", 125, 100, 180)\n",
    "weight = ROOT.RooRealVar(\"weight\",\"weight\",0,0,1)\n",
    "\n",
    "# Load the MC as RooDataSets, including the syst-varied TTrees\n",
    "mc = {}\n",
    "\n",
    "procs = ['ggH','VBF']\n",
    "cats = ['Tag0','Tag1']\n",
    "\n",
    "systs = ['scale','smear','JEC','photonID']\n",
    "\n",
    "for proc in procs:\n",
    "    for cat in cats:\n",
    "        key = \"%s_%s\"%(proc,cat)\n",
    "        t = f.Get(key)\n",
    "        mc[key] = ROOT.RooDataSet(key, key, t, ROOT.RooArgSet(mass,weight), \"\", \"weight\" )\n",
    "        for syst in systs:\n",
    "            for direction in ['Up','Down']:\n",
    "                key_syst = \"%s_%s%s01Sigma\"%(key,syst,direction)\n",
    "                t = f.Get(key_syst)\n",
    "                mc[key_syst] = ROOT.RooDataSet(key_syst, key_syst, t, ROOT.RooArgSet(mass,weight), \"\", \"weight\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variations in mean/sigma for the systematic-varied MC\n",
    "# Let's first build the model to fit\n",
    "mean = ROOT.RooRealVar(\"mean\", \"mean\", 125, 124, 126)\n",
    "sigma = ROOT.RooRealVar(\"sigma\", \"sigma\", 2, 1, 3)\n",
    "gaus = ROOT.RooGaussian(\"model\", \"model\", mass, mean, sigma)\n",
    "\n",
    "mean_values, sigma_values = {}, {}\n",
    "for proc in procs:\n",
    "    for cat in cats:\n",
    "        key = \"%s_%s\"%(proc,cat)\n",
    "        gaus.fitTo(mc[key], ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "        gaus.fitTo(mc[key], ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "        mean_values[key] = [mean.getVal(), mean.getError()]\n",
    "        sigma_values[key] = [sigma.getVal(), sigma.getError()]\n",
    "        for syst in ['scale','smear']:\n",
    "            key_up, key_down = \"%s_%sUp01Sigma\"%(key,syst), \"%s_%sDown01Sigma\"%(key,syst)\n",
    "            gaus.fitTo(mc[key_up], ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "            gaus.fitTo(mc[key_up], ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "            mean_values[key_up] = [mean.getVal(), mean.getError()]\n",
    "            sigma_values[key_up] = [sigma.getVal(), sigma.getError()]\n",
    "            gaus.fitTo(mc[key_down], ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "            gaus.fitTo(mc[key_down], ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "            mean_values[key_down] = [mean.getVal(), mean.getError()]\n",
    "            sigma_values[key_down] = [sigma.getVal(), sigma.getError()]\n",
    "\n",
    "# Store variations to bake into model\n",
    "scale_variations, smear_variations = {}, {}\n",
    "for proc in procs:\n",
    "    for cat in cats:\n",
    "        key = \"%s_%s\"%(proc,cat)\n",
    "        # Scale\n",
    "        syst = \"scale\"\n",
    "        key_up, key_down = \"%s_%sUp01Sigma\"%(key,syst), \"%s_%sDown01Sigma\"%(key,syst)\n",
    "        scale_variations[key] = 0.5*( abs(mean_values[key_up][0]/mean_values[key][0]-1) + abs(mean_values[key_down][0]/mean_values[key][0]-1) )\n",
    "        # Smear\n",
    "        syst = \"smear\"\n",
    "        key_up, key_down = \"%s_%sUp01Sigma\"%(key,syst), \"%s_%sDown01Sigma\"%(key,syst)\n",
    "        smear_variations[key] = 0.5*( abs(sigma_values[key_up][0]/sigma_values[key][0]-1) + abs(sigma_values[key_down][0]/sigma_values[key][0]-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets construct the signal models to save in the workspace\n",
    "MH = ROOT.RooRealVar(\"MH\", \"MH\", 125, 120, 130 )\n",
    "MH.setConstant(True)\n",
    "\n",
    "# Dicts to store the shape parameters\n",
    "dMH, mean_formula, sigma, sigma_formula = {}, {}, {}, {}\n",
    "models = {}\n",
    "\n",
    "# Also build nuisance parameters for scale and smearing effects\n",
    "eta = ROOT.RooRealVar(\"nuisance_scale\", \"nuisance_scale\", 0, -5, 5)\n",
    "eta.setConstant(True)\n",
    "chi = ROOT.RooRealVar(\"nuisance_smear\", \"nuisance_smear\", 0, -5, 5)\n",
    "chi.setConstant(True)\n",
    "\n",
    "for proc in procs:\n",
    "    for cat in cats:\n",
    "        key = \"%s_%s\"%(proc,cat)\n",
    "        dMH[key] = ROOT.RooRealVar(\"dMH_%s\"%key, \"dMH_%s\"%key, 0, -2, 2 )\n",
    "        mean_formula[key] = ROOT.RooFormulaVar(\"mean_%s\"%key, \"mean_%s\"%key, \"(@0+@1)*(1+%.4f*@2)\"%scale_variations[key], ROOT.RooArgList(MH,dMH[key],eta))\n",
    "\n",
    "        sigma[key] = ROOT.RooRealVar(\"sigma_%s_nominal\"%key, \"sigma_%s_nominal\"%key, 2, 1, 5)\n",
    "        sigma_formula[key] = ROOT.RooFormulaVar(\"sigma_%s\"%key, \"sigma_%s\"%key, \"@0*(1+%.4f*@1)\"%smear_variations[key], ROOT.RooArgList(sigma[key],chi))\n",
    "\n",
    "        models[key] = ROOT.RooGaussian( \"model_%s\"%key, \"model_%s\"%key, mass, mean_formula[key], sigma_formula[key] )\n",
    "\n",
    "        # Fit model to MC\n",
    "        models[key].fitTo( mc[key], ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "\n",
    "        # Set shape parameters of model to be constant (i.e. fixed in fit to data)\n",
    "        dMH[key].setConstant(True)\n",
    "        sigma[key].setConstant(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's build the normalisation objects\n",
    "\n",
    "# From LHCHWG twiki: https://twiki.cern.ch/twiki/bin/view/LHCPhysics/CERNYellowReportPageAt13TeV\n",
    "xs = {}\n",
    "xs['ggH'] = ROOT.RooRealVar(\"xs_ggH\", \"Cross section of ggH in [pb]\", 48.58 )\n",
    "xs['ggH'].setConstant(True)\n",
    "xs['VBF'] = ROOT.RooRealVar(\"xs_VBF\", \"Cross section of VBF in [pb]\", 3.782 )\n",
    "xs['VBF'].setConstant(True)\n",
    "\n",
    "br_gamgam = ROOT.RooRealVar(\"BR_gamgam\", \"Branching ratio of Higgs to gamma gamma\", 2.7e-3 )\n",
    "br_gamgam.setConstant(True)\n",
    "\n",
    "# Calculate the efficiency of selection for the different proc/cat combinations and define norm obkects\n",
    "eff, norms = {}, {}\n",
    "for proc in procs:\n",
    "    for cat in cats:\n",
    "        key = \"%s_%s\"%(proc,cat)\n",
    "        sumw = mc[key].sumEntries()\n",
    "        e = sumw/(xs[proc].getVal()*br_gamgam.getVal())\n",
    "        eff[key] = ROOT.RooRealVar(\"eff_%s\"%key, \"Efficiency for %s events to land in %s\"%(proc,cat), e )\n",
    "        # Set constant\n",
    "        eff[key].setConstant(True)\n",
    "        # Define normalisation objects\n",
    "        norms[key] = ROOT.RooProduct(\"model_%s_norm\"%key, \"Normalisation term for %s in %s\"%(proc,cat), ROOT.RooArgList(xs[proc],br_gamgam,eff[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = ROOT.TFile(\"workspace_sig_part6.root\", \"RECREATE\")\n",
    "w_sig = ROOT.RooWorkspace(\"workspace_sig\",\"workspace_sig\")\n",
    "for model in models.values(): getattr(w_sig, \"import\")(model)\n",
    "for norm in norms.values(): getattr(w_sig, \"import\")(norm)\n",
    "w_sig.Print()\n",
    "w_sig.Write()\n",
    "f_out.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background model construction\n",
    "# Lets build a RooMultiPdf to fit the background distribution in each category\n",
    "mass = ROOT.RooRealVar(\"CMS_hgg_mass\", \"CMS_hgg_mass\", 125, 100, 180)\n",
    "weight = ROOT.RooRealVar(\"weight\",\"weight\",0,0,1)\n",
    "\n",
    "# Define mass ranges to be fit to find the initial background model parameter values\n",
    "mass.setRange(\"loSB\", 100, 115 )\n",
    "mass.setRange(\"hiSB\", 135, 180 )\n",
    "mass.setRange(\"full\", 100, 180 )\n",
    "fit_range = \"loSB,hiSB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dicts to store the background model objects\n",
    "alpha, poly_1, poly_2, poly_3, poly_4, pow_1 = {}, {}, {}, {}, {}, {}\n",
    "pdfs = {}\n",
    "index = {}\n",
    "models_bkg = {}\n",
    "multipdfs = {}\n",
    "norms_bkg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file\n",
    "f = ROOT.TFile(\"data_part6.root\",\"r\")\n",
    "\n",
    "cats = ['Tag0','Tag1']\n",
    "\n",
    "data = {}\n",
    "for cat in cats:\n",
    "\n",
    "    # Build a RooDataSet for the category\n",
    "    t = f.Get(\"data_%s\"%cat)\n",
    "    data[cat] = ROOT.RooDataSet(\"data_%s\"%cat, \"data_%s\"%cat, t, ROOT.RooArgSet(mass), \"\", \"weight\")\n",
    "\n",
    "    alpha[cat] = ROOT.RooRealVar(\"alpha_%s\"%cat, \"alpha_%s\"%cat, -0.05, -0.2, 0 )\n",
    "    pdfs[\"%s_exp\"%cat] = ROOT.RooExponential(\"model_bkg_exp_%s\"%cat, \"model_bkg_exp_%s\"%cat, mass, alpha[cat] )\n",
    "    pdfs[\"%s_exp\"%cat].fitTo( data[cat], ROOT.RooFit.Range(fit_range), ROOT.RooFit.Minimizer(\"Minuit2\",\"minimize\"),ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "\n",
    "    poly_1[cat] = ROOT.RooRealVar(\"poly_1_%s\"%cat,\"poly_1_%s\"%cat, 0.01, -4, 4)\n",
    "    poly_2[cat] = ROOT.RooRealVar(\"poly_2_%s\"%cat,\"poly_2_%s\"%cat, 0.01, -4, 4)\n",
    "    poly_3[cat] = ROOT.RooRealVar(\"poly_3_%s\"%cat,\"poly_3_%s\"%cat, 0.01, -4, 4)\n",
    "    poly_4[cat] = ROOT.RooRealVar(\"poly_4_%s\"%cat,\"poly_4_%s\"%cat, 0.01, -4, 4)\n",
    "    pdfs[\"%s_poly\"%cat] = ROOT.RooChebychev(\"model_bkg_poly_%s\"%cat, \"model_bkg_poly_%s\"%cat, mass, ROOT.RooArgList(poly_1[cat],poly_2[cat],poly_3[cat],poly_4[cat]) )\n",
    "    pdfs[\"%s_poly\"%cat].fitTo( data[cat], ROOT.RooFit.Range(fit_range), ROOT.RooFit.Minimizer(\"Minuit2\",\"minimize\"),ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "\n",
    "    pow_1[cat] = ROOT.RooRealVar(\"pow_1_%s\"%cat,\"pow_1_%s\"%cat, -3, -10, -0.0001)\n",
    "    pdfs[\"%s_pow\"%cat] = ROOT.RooGenericPdf(\"model_bkg_pow_%s\"%cat, \"TMath::Power(@0,@1)\", ROOT.RooArgList(mass,pow_1[cat]) )\n",
    "    pdfs[\"%s_pow\"%cat].fitTo( data[cat], ROOT.RooFit.Range(fit_range), ROOT.RooFit.Minimizer(\"Minuit2\",\"minimize\"),ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "\n",
    "    # Make RooCategory index object to control which pdf is acitve\n",
    "    index[cat] = ROOT.RooCategory(\"pdfindex_%s\"%cat, \"Index of Pdf which is active for %s\"%cat )\n",
    "\n",
    "    # Make ArgList of models\n",
    "    models_bkg[cat] = ROOT.RooArgList()\n",
    "    models_bkg[cat].add( pdfs[\"%s_exp\"%cat] )\n",
    "    models_bkg[cat].add( pdfs[\"%s_poly\"%cat] )\n",
    "    models_bkg[cat].add( pdfs[\"%s_pow\"%cat] )\n",
    "\n",
    "    # Build the RooMultiPdf object\n",
    "    multipdfs[cat] = ROOT.RooMultiPdf(\"multipdf_%s\"%cat, \"Multipdf for %s\"%cat, index[cat], models_bkg[cat])\n",
    "\n",
    "    # As usual for the data driven fit we want the background to have a freely floating yield\n",
    "    norms_bkg[cat] = ROOT.RooRealVar(\"multipdf_%s_norm\"%cat, \"Number of background events in %s\"%cat, data[cat].numEntries(), 0, 3*data[cat].numEntries() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets again save the data as RooDataHist with 320 bins\n",
    "mass.setBins(320)\n",
    "data_hist = {}\n",
    "for cat in cats:\n",
    "    data_hist[cat] = ROOT.RooDataHist(\"data_hist_%s\"%cat, \"data_hist_%s\"%cat, mass, data[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save the background models and data sets to a workspace\n",
    "f_out = ROOT.TFile(\"workspace_bkg_part6.root\", \"RECREATE\")\n",
    "w_bkg = ROOT.RooWorkspace(\"workspace_bkg\",\"workspace_bkg\")\n",
    "for cat in cats:\n",
    "    getattr(w_bkg, \"import\")(data_hist[cat])\n",
    "    getattr(w_bkg, \"import\")(index[cat])\n",
    "    getattr(w_bkg, \"import\")(norms_bkg[cat])\n",
    "    getattr(w_bkg, \"import\")(multipdfs[cat])\n",
    "w_bkg.Print()\n",
    "w_bkg.Write()\n",
    "f_out.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datacards\n",
    "Firstly, we need to check the yield variations from the JEC and photonID systematics to add to the datacard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" --> Yield variations to add to the datacard...\")\n",
    "for proc in procs:\n",
    "    for cat in cats:\n",
    "        key = \"%s_%s\"%(proc,cat)\n",
    "        for syst in ['JEC','photonID']:\n",
    "            key_up, key_down = \"%s_%sUp01Sigma\"%(key,syst), \"%s_%sDown01Sigma\"%(key,syst)\n",
    "            yield_variation_up = mc[key_up].sumEntries()/mc[key].sumEntries()\n",
    "            yield_variation_down = mc[key_down].sumEntries()/mc[key].sumEntries()\n",
    "            print(\"Yield variation for %s systematic for (%s,%s): %.3f/%.3f\"%(syst,proc,cat,yield_variation_down,yield_variation_up))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datacards for the two analysis categories are saved separately as `datacard_part6_Tag0.txt` and `datacard_part6_Tag1.txt`. Lets open them up and have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's open the datacard and take a look\n",
    "with open(\"datacard_part6_Tag0.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(\"\".join(lines))\n",
    "\n",
    "print(\"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\n\")\n",
    "\n",
    "with open(\"datacard_part6_Tag1.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(\"\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do you understand the changes made to include multiple signal processes in the datacard? What value in the `process` line is used to label VBF as a signal?\n",
    "* Try compiling the individual datacards. What are the prefit ggH and VBF yields in each analysis category? You can find these by opening the workspace and printing the contents.\n",
    "* Run the best fits and plot the prefit and postfit S+B models along with the data (see code in part 2). How does the absolute number of data events in Tag1 compare to Tag0? What about the signal-to-background ratio, S/B? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the individual datacards here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the workspaces and print the contents. What are the ggH and VBF yields in each analysis category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the best fits and save the postfit snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the postfit S+B mgg distributions (using code from part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combine the two categories into a single datacard, we make use of the `combineCards.py` script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "combineCards.py datacard_part6_Tag0.txt datacard_part6_Tag1.txt > datacard_part6_combined.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the fits\n",
    "If we use the default `text2workspace` command on the combined datacard, then this will introduce a single signal strength modifer which modifies the rate of all signal processes (ggH and VBF) by the same factor. \n",
    "* Try compiling the combined datacard and running a likelihood scan. Does the sensitivity to the global signal strength improve by adding the additional analysis category \"Tag1\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the combined datacard into a RooWorkspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the likelihood scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scan with plot1DScan.py script: does the sensitivity improve by adding extra category?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to measure the independent rates of both processes simultaneously, then we need to introduce a separate signal strength for ggH and VBF. To do this we use the `multiSignalModel` physics model in combine by adding the following options to the `text2workspace` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "text2workspace.py datacard_part6_combined.txt -m 125 -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel \\\n",
    "--PO \"map=.*/ggH:r_ggH[1,0,2]\" --PO \"map=.*/VBF:r_VBF[1,0,3]\" -o datacard_part6_combined_multiSignalModel.root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax for the parameter to process mapping is `map=category/process/POI[default,min,max]`. We have used the wildcard `.*` to tell combine that the POI (parameter of interest) should scale all cases of that process, regardless of the analysis category. The output of the above command tells us what is scaled by the two signal strengths. You can see this is exactly what we require!\n",
    "\n",
    "To run a 1D \"profiled\" likelihood scan for ggH we use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "combine -M MultiDimFit datacard_part6_combined_multiSignalModel.root -m 125 --freezeParameters MH \\\n",
    "-n .scan.part6_multiSignalModel_ggH --algo grid --points 20 --cminDefaultMinimizerStrategy 0 \\\n",
    "--saveInactivePOI 1 -P r_ggH --floatOtherPOIs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"Profiled\" here means we are profiling over the other parameter of interest, `r_VBF` in the fit. In other words, we are treating `r_VBF` as an additional nuisance parameter. The option `--saveInactivePOI 1` stores the value of `r_VBF` in the combine output. Take a look at the fit output. Does the value of `r_VBF` depend on `r_ggH`? Are the two parameters of interest correlated? Remember, to look at the contents of the TTree you can use `limit->Show(i)`, where i is an integer labelling the point in the likelihood scan.\n",
    "* Run the profiled scan for the VBF signal strength. Plot the `r_ggH` and `r_VBF` likelihood scans using the `plot1DScan.py` script. You will need to change some of the input options, in particular the `--POI` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the fit output and print out the contents of the TTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run the profiled likelihood scan for r_VBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Plot the two profiled likelihood scans. You will need to change the options --POI in plot1DScan\n",
    "plot1DScan.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-dimensional likelihood scan\n",
    "We can also run the fit at fixed points in (`r_ggH`,`r_VBF`) space. By using a sufficient number of points, we are able to up the 2D likelihood surface. Let's change the ranges of the parameters of interest to match what we have found in the profiled scans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "combine -M MultiDimFit datacard_part6_combined_multiSignalModel.root -m 125 --freezeParameters MH \\\n",
    "-n .scan2D.part6_multiSignalModel --algo grid --points 800 --cminDefaultMinimizerStrategy 0 \\\n",
    "-P r_ggH -P r_VBF --setParameterRanges r_ggH=0.5,2.5:r_VBF=-1,2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the output you can use the following code (taken from `plot_2D_scan.py`). This code interpolates the 2NLL value between the points ran in the combine scan so that the plot shows a smooth likelihood surface. You may find in some cases, the number of scanned points and interpolation parameters need to be tuned to get a sensible looking surface. This basically depends on how complicated the likelihood surface is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "import numpy as np\n",
    "\n",
    "ROOT.gStyle.SetOptStat(0)\n",
    "\n",
    "# Open the combine output\n",
    "f = ROOT.TFile(\"higgsCombine.scan2D.part6_multiSignalModel.MultiDimFit.mH125.root\")\n",
    "t = f.Get(\"limit\")\n",
    "\n",
    "# Number of points in interpolation\n",
    "n_points = 1000\n",
    "x_range = [0.5,2.5]\n",
    "y_range = [-1,2]\n",
    "\n",
    "# Number of bins in plot\n",
    "n_bins = 40\n",
    "\n",
    "# Load the points and NLL from the combine output\n",
    "x, y, deltaNLL = [], [], []\n",
    "for ev in t:\n",
    "    x.append( getattr(ev,\"r_ggH\") )\n",
    "    y.append( getattr(ev,\"r_VBF\") )\n",
    "    deltaNLL.append( getattr(ev,\"deltaNLL\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do interpolation\n",
    "# Convert to numpy arrays as required for interpolation\n",
    "dnll = np.asarray(deltaNLL)\n",
    "points = np.array([x,y]).transpose()\n",
    "# Set up grid\n",
    "grid_x, grid_y = np.mgrid[x_range[0]:x_range[1]:n_points*1j, y_range[0]:y_range[1]:n_points*1j]\n",
    "grid_vals = griddata(points,dnll,(grid_x,grid_y), \"cubic\")\n",
    "\n",
    "# Remove NANS\n",
    "grid_x = grid_x[grid_vals==grid_vals]\n",
    "grid_y = grid_y[grid_vals==grid_vals]\n",
    "grid_vals = grid_vals[grid_vals==grid_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Profile2D histogram and fill\n",
    "h2D = ROOT.TProfile2D(\"h\",\"h\",n_bins,x_range[0],x_range[1],n_bins,y_range[0],y_range[1])\n",
    "for i in range(len(grid_vals)):\n",
    "  # Factor of 2 comes from 2*NLL\n",
    "  h2D.Fill( grid_x[i], grid_y[i], 2*grid_vals[i] )\n",
    "\n",
    "# Loop over bins: if content = 0 then set 999\n",
    "for ibin in range(1,h2D.GetNbinsX()+1):\n",
    "  for jbin in range(1,h2D.GetNbinsY()+1):\n",
    "    if h2D.GetBinContent(ibin,jbin)==0:\n",
    "      xc, yc = h2D.GetXaxis().GetBinCenter(ibin), h2D.GetYaxis().GetBinCenter(jbin)\n",
    "      h2D.Fill(xc,yc,999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up canvas\n",
    "canv = ROOT.TCanvas(\"canv\",\"canv\",600,600)\n",
    "canv.SetTickx()\n",
    "canv.SetTicky()\n",
    "canv.SetLeftMargin(0.115)\n",
    "canv.SetBottomMargin(0.115)\n",
    "# Extract binwidth\n",
    "xw = (x_range[1]-x_range[0])/n_bins\n",
    "yw = (y_range[1]-y_range[0])/n_bins\n",
    "\n",
    "# Set histogram properties\n",
    "h2D.SetContour(999)\n",
    "h2D.SetTitle(\"\")\n",
    "h2D.GetXaxis().SetTitle(\"r_ggH\")\n",
    "h2D.GetXaxis().SetTitleSize(0.05)\n",
    "h2D.GetXaxis().SetTitleOffset(0.9)\n",
    "h2D.GetXaxis().SetRangeUser(x_range[0],x_range[1]-xw)\n",
    "\n",
    "h2D.GetYaxis().SetTitle(\"r_VBF\")\n",
    "h2D.GetYaxis().SetTitleSize(0.05)\n",
    "h2D.GetYaxis().SetTitleOffset(0.9)\n",
    "h2D.GetYaxis().SetRangeUser(y_range[0],y_range[1]-yw)\n",
    "\n",
    "h2D.GetZaxis().SetTitle(\"-2 #Delta ln L\")\n",
    "h2D.GetZaxis().SetTitleSize(0.05)\n",
    "h2D.GetZaxis().SetTitleOffset(0.8)\n",
    "\n",
    "h2D.SetMaximum(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make confidence interval contours\n",
    "c68, c95 = h2D.Clone(), h2D.Clone()\n",
    "c68.SetContour(2)\n",
    "c68.SetContourLevel(1,2.3)\n",
    "c68.SetLineWidth(3)\n",
    "c68.SetLineColor(ROOT.kBlack)\n",
    "c95.SetContour(2)\n",
    "c95.SetContourLevel(1,5.99)\n",
    "c95.SetLineWidth(3)\n",
    "c95.SetLineStyle(2)\n",
    "c95.SetLineColor(ROOT.kBlack)\n",
    "\n",
    "# Draw histogram and contours\n",
    "h2D.Draw(\"COLZ\")\n",
    "\n",
    "# Draw lines for SM point\n",
    "vline = ROOT.TLine(1,y_range[0],1,y_range[1]-yw)\n",
    "vline.SetLineColorAlpha(ROOT.kGray,0.5)\n",
    "vline.Draw(\"Same\")\n",
    "hline = ROOT.TLine(x_range[0],1,x_range[1]-xw,1)\n",
    "hline.SetLineColorAlpha(ROOT.kGray,0.5)\n",
    "hline.Draw(\"Same\")\n",
    "\n",
    "# Draw contours\n",
    "c68.Draw(\"cont3same\")\n",
    "c95.Draw(\"cont3same\")\n",
    "\n",
    "# Make best fit and sm points\n",
    "gBF = ROOT.TGraph()\n",
    "gBF.SetPoint(0,grid_x[np.argmin(grid_vals)],grid_y[np.argmin(grid_vals)])\n",
    "gBF.SetMarkerStyle(34)\n",
    "gBF.SetMarkerSize(2)\n",
    "gBF.SetMarkerColor(ROOT.kBlack)\n",
    "gBF.Draw(\"P\")\n",
    "\n",
    "gSM = ROOT.TGraph()\n",
    "gSM.SetPoint(0,1,1)\n",
    "gSM.SetMarkerStyle(33)\n",
    "gSM.SetMarkerSize(2)\n",
    "gSM.SetMarkerColor(ROOT.kRed)\n",
    "gSM.Draw(\"P\")\n",
    "\n",
    "\n",
    "# Add legend\n",
    "leg = ROOT.TLegend(0.6,0.67,0.8,0.87)\n",
    "leg.SetBorderSize(0)\n",
    "leg.SetFillColor(0)\n",
    "leg.AddEntry(gBF,  \"Best fit\", \"P\" )\n",
    "leg.AddEntry(c68, \"1#sigma CL\" , \"L\" )\n",
    "leg.AddEntry(c95, \"2#sigma CL\" , \"L\" )\n",
    "leg.AddEntry(gSM,  \"SM\"     , \"P\" )\n",
    "leg.Draw()\n",
    "\n",
    "canv.Update()\n",
    "canv.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The plot shows that the data is in agreement with the SM within the $2\\sigma$ CL. Here, the $1\\sigma$ and $2\\sigma$ confidence interval contours corresponds to 2NLL values of 2.3 and 5.99, respectively. Do you understand why this? Think about Wilk's theorem.\n",
    "* Does the plot show any correlation between the ggH and VBF signal strengths? Are the two positively or negatively correlated? Does this make sense for this pair of parameters given the analysis setup? Try repeating the 2D likelihood scan using the \"Tag0\" only datacard. How does the correlation behaviour change?\n",
    "* How can we read off the \"profiled\" 1D likelihood scan constraints from this plot? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between parameters\n",
    "For template-based analyses we can use the `FitDiagnostics` method in combine to extract the covariance matrix for the fit parameters. Unfortunately, this method is not compatible when using discrete nuisance parameters (`RooMultiPdf`). Instead, we can use the `robustHesse` method to find the Hessian matrix by finite difference methods which iteratively removes NPs until the Hessian matrix is invertable. The matrix is then inverted to get the covariance. Subsequently, we can use the covariance to extract the correlations between fit parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "combine -M MultiDimFit datacard_part6_combined_multiSignalModel.root -m 125 --freezeParameters MH \\\n",
    "-n .robustHesse.part6_multiSignalModel --cminDefaultMinimizerStrategy 0 -P r_ggH -P r_VBF \\\n",
    "--setParameterRanges r_ggH=0.5,2.5:r_VBF=-1,2 --robustHesse 1 --robustHesseSave 1 --saveFitResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output file `robustHesse.robustHesse.part6_multiSignalModel.root` stores the correlation matrix (`h_correlation`). This contains the correlations between all parameters including the nuisances. So if we are interested in the correlation between `r_ggH` and `r_VBF`, we first need to find which bin corresponds to these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open combine output and load correlation matrix\n",
    "f = ROOT.TFile(\"robustHesse.robustHesse.part6_multiSignalModel.root\")\n",
    "h = f.Get(\"h_correlation\")\n",
    "\n",
    "# Find which indices correspond to the parameters of interest (square matrix)\n",
    "for i in range(1,h.GetNbinsX()+1):\n",
    "    print(i, h.GetXaxis().GetBinLabel(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correlation from bin content\n",
    "corr = h.GetBinContent(19,20)\n",
    "print(\"Correlation coefficient between r_ggH and r_VBF is: %.4f\"%corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The two parameters of interest have a correlation coefficient of -0.198. This means the two parameters are somewhat anti-correlated. Does this match what we see in the 2D likelihood scan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impacts\n",
    "We extract the impacts for each parameter of interest using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This block may take a while... don't panic!\n",
    "combineTool.py -M Impacts -d datacard_part6_combined_multiSignalModel.root -m 125 --freezeParameters MH \\\n",
    "-n .impacts_part6_multiSignal --robustFit 1 --cminDefaultMinimizerStrategy 0 -P r_ggH -P r_VBF --doInitialFit\n",
    "\n",
    "combineTool.py -M Impacts -d datacard_part6_combined_multiSignalModel.root -m 125 --freezeParameters MH \\\n",
    "-n .impacts_part6_multiSignal --robustFit 1 --cminDefaultMinimizerStrategy 0 -P r_ggH -P r_VBF --doFits\n",
    "\n",
    "combineTool.py -M Impacts -d datacard_part6_combined_multiSignalModel.root -m 125 --freezeParameters MH \\\n",
    "-n .impacts_part6_multiSignal --robustFit 1 --cminDefaultMinimizerStrategy 0 -P r_ggH -P r_VBF -o impacts_part6.json\n",
    "\n",
    "plotImpacts.py -i impacts_part6.json -o impacts_part6_r_ggH --POI r_ggH\n",
    "plotImpacts.py -i impacts_part6.json -o impacts_part6_r_VBF --POI r_VBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Examine the output PDF (and json) files. How does the impact ranking of the nuisance parameters change for the different signal strengths?\n",
    "\n",
    "## Advanced exercises (to be added... stay tuned)\n",
    "The combine experts will include additional exercises here in due course. These will include:\n",
    "* Convolution of model pdfs: `RooAddPdf`\n",
    "* Application of the spurious signal method\n",
    "* Advanced physics models including parametrised signal strengths e.g. SMEFT\n",
    "* Mass fits\n",
    "* Two-dimensional parametric models "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
