{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Discrete-profiling\n",
    "If multiple pdfs exist to fit some distribution, we can store all pdfs in a single workspace by using a `RooMultiPdf` object. The code blocks below show how to store the exponential, (4th order) Chebychev polynomial and the power law function from the previous section in a `RooMultiPdf` object. This requires a `RooCategory` index, which controls the pdf which is active at any one time. Look at the code and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mass and weight variables\n",
    "mass = ROOT.RooRealVar(\"CMS_hgg_mass\", \"CMS_hgg_mass\", 125, 100, 180)\n",
    "weight = ROOT.RooRealVar(\"weight\",\"weight\",0,0,1)\n",
    "\n",
    "# Load the data \n",
    "f = ROOT.TFile(\"data_part1.root\",\"r\")\n",
    "t = f.Get(\"data_Tag0\")\n",
    "\n",
    "data = ROOT.RooDataSet(\"data_Tag0\", \"data_Tag0\", t, ROOT.RooArgSet(mass), \"\", \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ranges to fit for initial parameter values\n",
    "mass.setRange(\"loSB\", 100, 115 )\n",
    "mass.setRange(\"hiSB\", 135, 180 )\n",
    "mass.setRange(\"full\", 100, 180 )\n",
    "fit_range = \"loSB,hiSB\"\n",
    "\n",
    "# Define the different background model pdf choices and fit to the data mass sidebands\n",
    "# RooExponential\n",
    "alpha = ROOT.RooRealVar(\"alpha\", \"alpha\", -0.05, -0.2, 0 )\n",
    "model_exp_bkg = ROOT.RooExponential(\"model_exp_bkg_Tag0\", \"model_exp_bkg_Tag0\", mass, alpha )\n",
    "# Fit model to data sidebands\n",
    "model_exp_bkg.fitTo( data, ROOT.RooFit.Range(fit_range), ROOT.RooFit.Minimizer(\"Minuit2\",\"minimize\"),ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "\n",
    "# RooChebychev polynomial: 4th order\n",
    "poly_1 = ROOT.RooRealVar(\"poly_1\",\"T1 of chebychev polynomial\", 0.01, -4, 4)\n",
    "poly_2 = ROOT.RooRealVar(\"poly_2\",\"T2 of chebychev polynomial\", 0.01, -4, 4)\n",
    "poly_3 = ROOT.RooRealVar(\"poly_3\",\"T3 of chebychev polynomial\", 0.01, -4, 4)\n",
    "poly_4 = ROOT.RooRealVar(\"poly_4\",\"T4 of chebychev polynomial\", 0.01, -4, 4)\n",
    "model_poly_bkg = ROOT.RooChebychev(\"model_poly_bkg_Tag0\", \"model_poly_bkg_Tag0\", mass, ROOT.RooArgList(poly_1,poly_2,poly_3,poly_4) )\n",
    "# Fit model to data sidebands\n",
    "model_poly_bkg.fitTo( data, ROOT.RooFit.Range(fit_range), ROOT.RooFit.Minimizer(\"Minuit2\",\"minimize\"),ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )\n",
    "\n",
    "# Power law function: using RooGenericPdf functionality\n",
    "pow_1 = ROOT.RooRealVar(\"pow_1\",\"Exponent of power law\", -3, -10, -0.0001)\n",
    "model_pow_bkg = ROOT.RooGenericPdf(\"model_pow_bkg_Tag0\", \"TMath::Power(@0,@1)\", ROOT.RooArgList(mass,pow_1) )\n",
    "# Fit model to data sidebands\n",
    "model_pow_bkg.fitTo( data, ROOT.RooFit.Range(fit_range), ROOT.RooFit.Minimizer(\"Minuit2\",\"minimize\"),ROOT.RooFit.SumW2Error(True), ROOT.RooFit.PrintLevel(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a RooCategory object: this will control which PDF is \"active\"\n",
    "cat = ROOT.RooCategory(\"pdfindex_Tag0\", \"Index of Pdf which is active for Tag0\")\n",
    "\n",
    "# Make a RooArgList of the models\n",
    "models = ROOT.RooArgList()\n",
    "models.add(model_exp_bkg)\n",
    "models.add(model_poly_bkg)\n",
    "models.add(model_pow_bkg)\n",
    "\n",
    "# Build the RooMultiPdf object\n",
    "multipdf = ROOT.RooMultiPdf(\"multipdf_Tag0\", \"MultiPdf for Tag0\", cat, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define normalisation object\n",
    "# As usual, data-driven fit so want the background model to have a freely floating yield\n",
    "norm = ROOT.RooRealVar(\"multipdf_Tag0_norm\", \"Number of background events in Tag0\", data.numEntries(), 0, 3*data.numEntries() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets save the data as a RooDataHist with 320 bins between 100 and 180\n",
    "mass.setBins(320)\n",
    "data_hist = ROOT.RooDataHist(\"data_hist_Tag0\", \"data_hist_Tag0\", mass, data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the background model and data set to a RooWorkspace\n",
    "f_out = ROOT.TFile(\"workspace_bkg_multipdf.root\", \"RECREATE\")\n",
    "w_bkg = ROOT.RooWorkspace(\"workspace_bkg\",\"workspace_bkg\")\n",
    "getattr(w_bkg, \"import\")(data_hist)\n",
    "getattr(w_bkg, \"import\")(cat)\n",
    "getattr(w_bkg, \"import\")(norm)\n",
    "getattr(w_bkg, \"import\")(multipdf)\n",
    "w_bkg.Print()\n",
    "w_bkg.Write()\n",
    "f_out.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `datacard_part5.txt` will load the multipdf as the background model. Notice the line at the end of the datacard (see below). This tells combine about the `RooCategory` index.\n",
    "```\n",
    "pdfindex_Tag0         discrete\n",
    "```\n",
    "First have a look at the datacard and then compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's open the datacard and take a look\n",
    "with open(\"datacard_part5.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(\"\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "text2workspace.py datacard_part5.txt -m 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RooMultiPdf` is a handy object for performing bias studies as all functions can be stored in a single workspace. You can then set which function is used for generating the toys with the `--setParameters pdfindex=i` option, and which function is used for fitting with `--setParameters pdfindex=i --freezeParameters pdfindex=j` options. \n",
    "* It would be a useful exercise (if time permits) to repeat the bias studies from part 4 but using the RooMultiPdf workspace. What happens when you do not freeze the index in the fitting step?\n",
    "\n",
    "But simpler bias studies are not the only benefit of using the `RooMultiPdf`! It also allows us to apply the [discrete profiling method](https://arxiv.org/pdf/1408.6865.pdf) in our analysis. In this method, the index labelling which pdf is active (a discrete nuisance parameter) is left floating in the fit, and will be profiled by looping through all the possible index values and finding the pdf which gives the best fit. In this manner, we are able to account for the **uncertainty in the choice of the background function**. \n",
    "\n",
    "Note, by default, the multipdf will tell combine to add 0.5 to the NLL for each parameter in the pdf. This is known as the penalty term (or correction factor) for the discrete profiling method. You can toggle this term when building the workspace with the command `multipdf.setCorrectionFactor(0.5)`. You may need to change the value of this term to obtain an acceptable bias in your fit!\n",
    "\n",
    "Let's run a likelihood scan using the compiled datacard with the `RooMultiPdf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "combine -M MultiDimFit datacard_part5.root -m 125 --freezeParameters MH \\\n",
    "-n .scan.multidimfit --algo grid --points 20 --cminDefaultMinimizerStrategy 0 \\\n",
    "--saveSpecifiedIndex pdfindex_Tag0 --setParameterRanges r=0.5,2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The option `--cminDefaultMinimizerStrategy 0` is required to prevent HESSE being called as this cannot handle discrete nuisance parameters. HESSE is the full calculation of the second derivative matrix (Hessian) of the likelihood using finite difference methods.\n",
    "\n",
    "The option `--saveSpecifiedIndex pdfindex_Tag0` saves the value of the index at each point in the likelihood scan. Let's have a look at how the index value changes as a function of the signal strength. You can make the following plot by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file with fits\n",
    "f = ROOT.TFile(\"higgsCombine.scan.multidimfit.MultiDimFit.mH125.root\")\n",
    "t = f.Get(\"limit\")\n",
    "\n",
    "r, pdfindex = [], []\n",
    "\n",
    "for ev in t:\n",
    "    r.append( getattr(ev,\"r\") )\n",
    "    pdfindex.append( getattr(ev,\"pdfindex_Tag0\") )\n",
    "\n",
    "gr = ROOT.TGraph()\n",
    "for i in range( len(r) ):\n",
    "    gr.SetPoint( gr.GetN(), r[i], pdfindex[i] )\n",
    "\n",
    "gr.GetXaxis().SetTitle(\"r\")\n",
    "gr.GetYaxis().SetTitle(\"pdfindex_Tag0\")\n",
    "\n",
    "gr.SetMarkerStyle(20)\n",
    "gr.SetMarkerSize(1.5)\n",
    "gr.SetLineWidth(0)\n",
    "\n",
    "can = ROOT.TCanvas()\n",
    "gr.Draw()\n",
    "\n",
    "can.Update()\n",
    "can.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By floating the discrete nuisance parameter `pdfindex_Tag0`, at each point in the likelihood scan the pdfs will be iterated over and the one which gives the max likelihood (lowest 2NLL) including the correction factor will be used. The plot above shows that the `pdfindex_Tag0=0` (exponential) is chosen for the majority of r values, but this switches to `pdfindex_Tag0=1` (Chebychev polynomial) at the lower edge of the r range. We can see the impact on the likelihood scan by fixing the background pdf to the exponential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "combine -M MultiDimFit datacard_part5.root -m 125 --freezeParameters MH,pdfindex_Tag0 \\\n",
    "--setParameters pdfindex_Tag0=0 -n .scan.multidimfit.fix_exp --algo grid --points 20 \\\n",
    "--cminDefaultMinimizerStrategy 0 --saveSpecifiedIndex pdfindex_Tag0 --setParameterRanges r=0.5,2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the two scans on the same axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "plot1DScan.py higgsCombine.scan.multidimfit.MultiDimFit.mH125.root --main-label \"Pdf choice floating\" \\\n",
    "--main-color 1 --others higgsCombine.scan.multidimfit.fix_exp.MultiDimFit.mH125.root:\"Pdf fixed to exponential\":2 \\\n",
    "-o part5_scan --y-cut 35 --y-max 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets open the png file and plot it here\n",
    "Image(filename='part5_scan.png', width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impact on the likelihood scan is evident at the lower edge, where the scan in which the index is floating flattens out. In this example, neither the $1\\sigma$ or $2\\sigma$ intervals are affected. But this is not always the case! Ultimately, this method allows us to account for the uncertainty in the choice of background function in the signal strength measurement. \n",
    "\n",
    "Coming back to the bias studies. Do you now understand what you are testing if you do not freeze the index in the fitting stage? In this case you are fitting the toys back with the discrete profiling method. This is the standard approach for the bias studies when we use the discrete-profiling method in an analysis.\n",
    "\n",
    "There are a number of options which can be added to the combine command to improve the performance when using discrete nuisance parameters. These are detailed at the end of this [section](https://cms-analysis.github.io/HiggsAnalysis-CombinedLimit/part3/nonstandard/#discrete-profiling) in the combine manual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
